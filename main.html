<!doctype html>
<html lang="en">
<head>
	<meta charset="utf8" />
	<title>ML Blog post</title>
</head>
<body style='width:40%;padding-left: 30%; display: block;text-align: justify;'>
<h1 class="unnumbered" id="why-perceptrons">Why perceptrons?</h1>
<p>Any machine learning algorithm must follow the general purpose recipe
for machine learning, as this recipe defines the process by which a
machine learning model can converge to being an accurate model for the
problem (i.e. an accurate classifier). The recipe is as follows:</p>
<ol>
<li><p>Collect a dataset</p></li>
<li><p>Choose a decision function</p></li>
<li><p>Construct a loss function</p></li>
<li><p>Define goal (based on loss function)</p></li>
<li><p>Optimize our guessing to minimize the cost function</p></li>
</ol>
<p>For the specific case of perceptrons, we have chosen the following
inputs to our recipe</p>
<ol>
<li><p>Collect a dataset</p>
<p><img style="vertical-align:middle"
src="https://latex.codecogs.com/png.latex?%5Cdisplaystyle%20%5C%7Bx%5Ei%2C%20y%5Ei%5C%7D_%7Bi%3D1%7D%5EN"
alt="\{x^i, y^i\}_{i=1}^N" title="\{x^i, y^i\}_{i=1}^N"
class="math display" /></p></li>
<li><p>Choose a decision function</p>
<p><img style="vertical-align:middle"
src="https://latex.codecogs.com/png.latex?%5Cdisplaystyle%20%5Chat%7By%7D%20%3D%20f_%5Ctheta%28x%29"
alt="\hat{y} = f_\theta(x)" title="\hat{y} = f_\theta(x)"
class="math display" /></p></li>
<li><p>Construct a loss function</p>
<p><img style="vertical-align:middle"
src="https://latex.codecogs.com/png.latex?%5Cdisplaystyle%20l%28%5Chat%20%7By%7D%5Ei%2C%20y%5Ei%29"
alt="l(\hat {y}^i, y^i)" title="l(\hat {y}^i, y^i)"
class="math display" /></p></li>
<li><p>Define goal:</p>
<p><img style="vertical-align:middle"
src="https://latex.codecogs.com/png.latex?%5Cdisplaystyle%20%5Ctheta%5E%2A%20%3D%20%5Carg%20%5Cmin_%5Ctheta%20%5CSigma_%7Bi%20%3D%201%7D%5EN%20l%28f_%5Ctheta%28x%5Ei%29%2C%20y_i%29"
alt="\theta^* = \arg \min_\theta \Sigma_{i = 1}^N l(f_\theta(x^i), y_i)"
title="\theta^* = \arg \min_\theta \Sigma_{i = 1}^N l(f_\theta(x^i), y_i)"
class="math display" /></p></li>
<li><p>Optimize our guessing to minimize the cost function: Train with
stochastic gradient descent!</p></li>
</ol>
<h1 class="unnumbered" id="what-are-the-problems-with-perceptrons">What
are the problems with perceptrons?</h1>
<p>Previously we had that at each node,</p>
<p><img style="vertical-align:middle"
src="https://latex.codecogs.com/png.latex?%5Cdisplaystyle%20%5Csigma%28%5CSigma_%7Bi%20%3D%200%7D%5Em%20x_iw_%3Ai%29%20-%20%5Csigma%28x%5E%5Ctop%20w%29%20-%20%5Chat%7By%7D"
alt="\sigma(\Sigma_{i = 0}^m x_iw_:i) - \sigma(x^\top w) - \hat{y}"
title="\sigma(\Sigma_{i = 0}^m x_iw_:i) - \sigma(x^\top w) - \hat{y}"
class="math display" /></p>
<p>where <img style="vertical-align:middle"
src="https://latex.codecogs.com/png.latex?%5Ctextstyle%20%5Csigma%28x%29%20%3D%200"
alt="\sigma(x) = 0" title="\sigma(x) = 0" class="math inline" /> if <img
style="vertical-align:middle"
src="https://latex.codecogs.com/png.latex?%5Ctextstyle%20z%20%5Cleq%200"
alt="z \leq 0" title="z \leq 0" class="math inline" /> or 1 if <img
style="vertical-align:middle"
src="https://latex.codecogs.com/png.latex?%5Ctextstyle%20x%20%5Cgeq%200"
alt="x \geq 0" title="x \geq 0" class="math inline" /></p>
<p>Obviously this activation function is non-differentiable with a huge
jump continuity at <img style="vertical-align:middle"
src="https://latex.codecogs.com/png.latex?%5Ctextstyle%20x%20%3D%200"
alt="x = 0" title="x = 0" class="math inline" />. This makes it very
difficult for us to apply gradient descent, and thus hampers our ability
to actually optimize classical perceptrons with mathematical
techniques.</p>
<p>We want to change the perceptrons for us to be able to turn them into
a format that can be handled by standard optimization techniques. We can
still model each node (which is supposed to resemble a neuron in the
human brain) as having some number of inputs and summing those up to
produce an output but that output need not follow an activation
function.</p>
<p>We can try to redefine our previous version of perceptrons:</p>
<p>Initially we can start off our perceptron with the following:</p>
<p><img style="vertical-align:middle"
src="https://latex.codecogs.com/png.latex?%5Cdisplaystyle%20%5Ctext%7BLet%20%7DD%20%5Cequiv%20%5C%7Bx%5Ei%2C%20y%5Ei%5C%7D_%7Bi%20%3D%201%7D%5EN"
alt="\text{Let }D \equiv \{x^i, y^i\}_{i = 1}^N"
title="\text{Let }D \equiv \{x^i, y^i\}_{i = 1}^N"
class="math display" /></p>
<p><img style="vertical-align:middle"
src="https://latex.codecogs.com/png.latex?%5Cdisplaystyle%20%5Cvec%7Bw%7D%5E0%20%3D%200%5Ed"
alt="\vec{w}^0 = 0^d" title="\vec{w}^0 = 0^d"
class="math display" /></p>
<p>For every training epoch, every <img style="vertical-align:middle"
src="https://latex.codecogs.com/png.latex?%5Ctextstyle%20%28x%5Ei%2C%20y%5Ei%29%20%5Cin%20D"
alt="(x^i, y^i) \in D" title="(x^i, y^i) \in D"
class="math inline" />:</p>
<p><img style="vertical-align:middle"
src="https://latex.codecogs.com/png.latex?%5Cdisplaystyle%20%5Chat%7By%7D%5Ei%20%3D%20%5Csigma%28%5Cvec%7Bw%7D%5E%5Ctop%20x%5Ei%29"
alt="\hat{y}^i = \sigma(\vec{w}^\top x^i)"
title="\hat{y}^i = \sigma(\vec{w}^\top x^i)" class="math display" /></p>
<p><img style="vertical-align:middle"
src="https://latex.codecogs.com/png.latex?%5Cdisplaystyle%20e%20%3D%20%28y%5Ei%20-%20%5Chat%7By%7D%5Ei%29"
alt="e = (y^i - \hat{y}^i)" title="e = (y^i - \hat{y}^i)"
class="math display" /></p>
<p><img style="vertical-align:middle"
src="https://latex.codecogs.com/png.latex?%5Cdisplaystyle%20%5Cvec%7Bw%7D%5E%7Bi%20%2B%201%7D%20%5Cgets%20%5Cvec%7Bw%7D%5Ei%20%2B%20e%20%5Ctimes%20x%5Ei"
alt="\vec{w}^{i + 1} \gets \vec{w}^i + e \times x^i"
title="\vec{w}^{i + 1} \gets \vec{w}^i + e \times x^i"
class="math display" /></p>
<p>However, we still need to figure out a choice of <img
style="vertical-align:middle"
src="https://latex.codecogs.com/png.latex?%5Ctextstyle%20%5Csigma"
alt="\sigma" title="\sigma" class="math inline" /> to make the function
defined by our perceptron differentiable.</p>
<p>Previously we were using an activation function: <img
style="vertical-align:middle"
src="https://latex.codecogs.com/png.latex?%5Ctextstyle%20y%20%3D%20%5Ctext%7Bsign%7D%28%5Cvec%7Bw%7D%5ET%20x%29"
alt="y = \text{sign}(\vec{w}^T x)" title="y = \text{sign}(\vec{w}^T x)"
class="math inline" />. As mentioned numerous times before this is not
differentiable.</p>
<p>Another big issue is that there are no nonlinear boundaries possible
with classical perceptrons; perceptrons donâ€™t converge when classes are
linearly nonseparable. Consider the function <img
style="vertical-align:middle"
src="https://latex.codecogs.com/png.latex?%5Ctextstyle%20%5Ctext%7Bsign%7D%28x%20%2A%20y%29"
alt="\text{sign}(x * y)" title="\text{sign}(x * y)"
class="math inline" />. As no single line can separate all of the values
in 2 diagonally adjacent quadrants from all of the values in the other 2
quadrants, we have a fundamental issue in the flexibility of
perceptrons.</p>
<h1 class="unnumbered" id="how-do-we-fix-perceptrons">How do we fix
perceptrons?</h1>
<p>In order to make these perceptrons differentiable and thus
optimizable, we need to find some function that is differentiable but
increasing and defined on a bounded domain. Looking back on simpler
machine learning techniques, we already had such a function with our
logistic regression classifier! We take our logistic regression
classifier function and use that to determine the output of a node given
its inputs:</p>
<p><img style="vertical-align:middle"
src="https://latex.codecogs.com/png.latex?%5Cdisplaystyle%20y%20%3D%20%5Cfrac%7B1%7D%7B1%20%2B%20e%5E%7B-%5Cvec%7Bw%7D%5E%5Ctop%20x%7D%7D"
alt="y = \frac{1}{1 + e^{-\vec{w}^\top x}}"
title="y = \frac{1}{1 + e^{-\vec{w}^\top x}}"
class="math display" /></p>
<p>Our full definition of our new and improved differentiable
perceptrons can thus be:</p>
<p>Input data:</p>
<p><img style="vertical-align:middle"
src="https://latex.codecogs.com/png.latex?%5Cdisplaystyle%20X%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Bd%20%5Ctimes%20n%7D"
alt="X \in \mathbb{R}^{d \times n}"
title="X \in \mathbb{R}^{d \times n}" class="math display" /></p>
<p><img style="vertical-align:middle"
src="https://latex.codecogs.com/png.latex?%5Cdisplaystyle%20Y%20%5Cin%20%5C%7B0%2C%201%5C%7D%5En"
alt="Y \in \{0, 1\}^n" title="Y \in \{0, 1\}^n"
class="math display" /></p>
<p>where</p>
<p><img style="vertical-align:middle"
src="https://latex.codecogs.com/png.latex?%5Cdisplaystyle%20%28%5Cvec%7Bx%7D%20%5Cin%20%5Cmathbb%7BR%7D%5Ed%2C%20y%5Cin%20%5C%7B0%2C%201%5C%7D%29"
alt="(\vec{x} \in \mathbb{R}^d, y\in \{0, 1\})"
title="(\vec{x} \in \mathbb{R}^d, y\in \{0, 1\})"
class="math display" /></p>
<p>corresponds to a data point</p>
<p><img style="vertical-align:middle"
src="https://latex.codecogs.com/png.latex?%5Cdisplaystyle%20p%28y%20%3D%201%7Cx%29%20%3D%20%5Cfrac%7B1%7D%7B1%20%2B%20e%5E%7B-%5Cvec%7Bw%7D%5E%5Ctop%20x%7D%7D"
alt="p(y = 1|x) = \frac{1}{1 + e^{-\vec{w}^\top x}}"
title="p(y = 1|x) = \frac{1}{1 + e^{-\vec{w}^\top x}}"
class="math display" /></p>
<p>With an objective function of</p>
<p><img style="vertical-align:middle"
src="https://latex.codecogs.com/png.latex?%5Cdisplaystyle%20%5Carg%20%5Cmin_%7B%5Cvec%7Bw%7D%7D%20L%28%5Cvec%7Bw%7D%3B%20D%29"
alt="\arg \min_{\vec{w}} L(\vec{w}; D)"
title="\arg \min_{\vec{w}} L(\vec{w}; D)" class="math display" /></p>
<h1 class="unnumbered" id="what-to-do-with-this-information">What to do
with this information</h1>
<p>We can solve this new version of a perceptron by using a technique
such as stochastic gradient descent, thus allowing us to get a very good
approximator for clasification problems</p>
<p>Relatedly, for logistic regresion with gradients: loss is negative
log likelihood of the probability that our perceptron classifies our
data correctly.</p>
<p><img style="vertical-align:middle"
src="https://latex.codecogs.com/png.latex?%5Cdisplaystyle%20y%5Ei%20%3D%201%2C%20l%28x%5Ei%2C%20y%5Ei%29%20%3D%20-%5Cln%28p%5Ei%29"
alt="y^i = 1, l(x^i, y^i) = -\ln(p^i)"
title="y^i = 1, l(x^i, y^i) = -\ln(p^i)" class="math display" /></p>
<p><img style="vertical-align:middle"
src="https://latex.codecogs.com/png.latex?%5Cdisplaystyle%20y%5Ei%20%3D%200%2C%20l%28x%5Ei%2C%20y%5Ei%29%20%3D%20-%5Cln%281%20-%20p%5Ei%29"
alt="y^i = 0, l(x^i, y^i) = -\ln(1 - p^i)"
title="y^i = 0, l(x^i, y^i) = -\ln(1 - p^i)" class="math display" /></p>
<p>So we can unify these into one function:</p>
<p><img style="vertical-align:middle"
src="https://latex.codecogs.com/png.latex?%5Cdisplaystyle%20l%28x%5Ei%2C%20y%5Ei%29%20%3D%20-y%5Ei%20%5Cln%28p%5Ei%29%20-%20%281%20-%20y%5Ei%29%5Cln%281%20-%20p%5Ei%29"
alt="l(x^i, y^i) = -y^i \ln(p^i) - (1 - y^i)\ln(1 - p^i)"
title="l(x^i, y^i) = -y^i \ln(p^i) - (1 - y^i)\ln(1 - p^i)"
class="math display" /></p>
<p>We can try to apply this to data like the standard handwritten digits
dataset.</p>
<h1 class="unnumbered" id="problems-that-remain">Problems that
remain</h1>
<p>We still havenâ€™t really fixed the linear separability issue. We also
still have a very wasteful way of calculating information: the
brightness values of every single pixel in a large image will result in
a huge number of inputs that we need to feed into a perceptron making
stochastic gradient descent here likely prohibitively costly. Thus, we
want to find some way to condense the information we feed into the
perceptron into a small set of relevant, information dense inputs. We
call these inputs features.</p>
<p>In other words we can have many inputs which arenâ€™t individually
descriptive, so we may end up "featurizing" the input to use as input to
train the perceptron. However, this process of "featurizing" can itself
become a machine learning problem: how do we choose and define
features?</p>
<p>We could potentially choose features through a perceptron! This is
akin to hooking one perceptron with n inputs and m outputs into a
perceptron with m inputs and <img style="vertical-align:middle"
src="https://latex.codecogs.com/png.latex?%5Ctextstyle%20%5Cell"
alt="\ell" title="\ell" class="math inline" /> outputs. We connect each
"neuron" in the first layer of this fused perceptron, or "neural
network" to each "neuron" in the second layer of the fused perceptron
with a weight. This allows for a very expressive, efficient, and flexible
machine learning framework which will be explained further in later
lectures!</p>
</body>
